{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Mario.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"msiBaxqyfCzt","executionInfo":{"status":"ok","timestamp":1620254471883,"user_tz":300,"elapsed":4176,"user":{"displayName":"Matthew Choi","photoUrl":"","userId":"14303575451927255395"}},"outputId":"3d5dc9e0-c36f-425c-8037-21ed21aca33c"},"source":["import time\n","import random\n","import numpy as np\n","from collections import deque\n","!pip install tensorflow==1.13.1\n","import tensorflow as tf\n","from matplotlib import pyplot as plt"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.7/dist-packages (1.13.1)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.3.3)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.12.0)\n","Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.13.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.19.5)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.12.4)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.32.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.0.8)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.36.2)\n","Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.13.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (56.0.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n","Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.0.3)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.10.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"5mOiRTimsg20","executionInfo":{"status":"ok","timestamp":1620254472416,"user_tz":300,"elapsed":4698,"user":{"displayName":"Matthew Choi","photoUrl":"","userId":"14303575451927255395"}}},"source":["class DQNAgent:\n","    \"\"\" DQN agent \"\"\"\n","    def __init__(self, states, actions, max_memory, double_q):\n","        self.states = states\n","        self.actions = actions\n","        self.session = tf.Session()\n","        self.build_model()\n","        self.saver = tf.train.Saver(max_to_keep=10)\n","        self.session.run(tf.global_variables_initializer())\n","        self.saver = tf.train.Saver()\n","        self.memory = deque(maxlen=max_memory)\n","        self.eps = 1\n","        self.eps_decay = 0.99999975\n","        self.eps_min = 0.1\n","        self.gamma = 0.90\n","        self.batch_size = 32\n","        self.burnin = 100000\n","        self.copy = 10000\n","        self.step = 0\n","        self.learn_each = 3\n","        self.learn_step = 0\n","        self.save_each = 500000\n","        self.double_q = double_q\n","\n","    def build_model(self):\n","        \"\"\" Model builder function \"\"\"\n","        self.input = tf.placeholder(dtype=tf.float32, shape=(None, ) + self.states, name='input')\n","        self.q_true = tf.placeholder(dtype=tf.float32, shape=[None], name='labels')\n","        self.a_true = tf.placeholder(dtype=tf.int32, shape=[None], name='actions')\n","        self.reward = tf.placeholder(dtype=tf.float32, shape=[], name='reward')\n","        self.input_float = tf.to_float(self.input) / 255.\n","        # Online network\n","        '''\n","        online_model = keras.Sequential()\n","        online_model.add(keras.layers.Conv2D(filters=32, kernel_size=8, strides=4, activation=tf.nn.relu, input_shape = self.input_float.shape()))\n","        online_model.add(keras.layers.Conv2D(filters=64, kernel_size=4, strides=2, activation=tf.nn.relu)\n","        online_model.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, activation=tf.nn.relu)\n","        online_model.add(Flatten())\n","        online_model.add(Dense(units = 512, activation=tf.nn.relu))\n","        online_model.add(Dense(units = self.actions), name = \"output\")\n","        '''\n","        '''\n","        target_model = keras.Sequential()\n","        target_model.add(keras.layers.Conv2D(filters=32, kernel_size=8, strides=4, activation=tf.nn.relu, input_shape = self.input_float.shape()))\n","        target_model.add(keras.layers.Conv2D(filters=64, kernel_size=4, strides=2, activation=tf.nn.relu)\n","        target_model.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=1, activation=tf.nn.relu)\n","        target_model.add(Flatten())\n","        target_model.add(Dense(units = 512, activation=tf.nn.relu))\n","        target_model.add(Dense(units = self.actions), name = \"output_target\")\n","\n","        # Compiling the CNN\n","        model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n","\n","        # Fitting the CNN to the Training set\n","        model.fit(X_train, y_train, epochs = 100, batch_size = 32)\n","        '''\n","        with tf.variable_scope('online'):\n","            self.conv_1 = tf.layers.conv2d(inputs=self.input_float, filters=32, kernel_size=8, strides=4, activation=tf.nn.relu)\n","            self.conv_2 = tf.layers.conv2d(inputs=self.conv_1, filters=64, kernel_size=4, strides=2, activation=tf.nn.relu)\n","            self.conv_3 = tf.layers.conv2d(inputs=self.conv_2, filters=64, kernel_size=3, strides=1, activation=tf.nn.relu)\n","            self.flatten = tf.layers.flatten(inputs=self.conv_3)\n","            self.dense = tf.layers.dense(inputs=self.flatten, units=512, activation=tf.nn.relu)\n","            self.output = tf.layers.dense(inputs=self.dense, units=self.actions, name='output')\n","        # Target network\n","        with tf.variable_scope('target'):\n","            self.conv_1_target = tf.layers.conv2d(inputs=self.input_float, filters=32, kernel_size=8, strides=4, activation=tf.nn.relu)\n","            self.conv_2_target = tf.layers.conv2d(inputs=self.conv_1_target, filters=64, kernel_size=4, strides=2, activation=tf.nn.relu)\n","            self.conv_3_target = tf.layers.conv2d(inputs=self.conv_2_target, filters=64, kernel_size=3, strides=1, activation=tf.nn.relu)\n","            self.flatten_target = tf.layers.flatten(inputs=self.conv_3_target)\n","            self.dense_target = tf.layers.dense(inputs=self.flatten_target, units=512, activation=tf.nn.relu)\n","            self.output_target = tf.stop_gradient(tf.layers.dense(inputs=self.dense_target, units=self.actions, name='output_target'))\n","        # Optimizer\n","        self.action = tf.argmax(input=self.output, axis=1)\n","        self.q_pred = tf.gather_nd(params=self.output, indices=tf.stack([tf.range(tf.shape(self.a_true)[0]), self.a_true], axis=1))\n","        self.loss = tf.losses.huber_loss(labels=self.q_true, predictions=self.q_pred)\n","        self.train = tf.train.AdamOptimizer(learning_rate=0.00025).minimize(self.loss)\n","        # Summaries\n","        self.summaries = tf.summary.merge([\n","            tf.summary.scalar('reward', self.reward),\n","            tf.summary.scalar('loss', self.loss),\n","            tf.summary.scalar('max_q', tf.reduce_max(self.output))\n","        ])\n","        self.writer = tf.summary.FileWriter(logdir='./logs', graph=self.session.graph)\n","\n","    def copy_model(self):\n","        \"\"\" Copy weights to target network \"\"\"\n","        self.session.run([tf.assign(new, old) for (new, old) in zip(tf.trainable_variables('target'), tf.trainable_variables('online'))])\n","\n","    def save_model(self):\n","        \"\"\" Saves current model to disk \"\"\"\n","        self.saver.save(sess=self.session, save_path='./models/model', global_step=self.step)\n","\n","    def add(self, experience):\n","        \"\"\" Add observation to experience \"\"\"\n","        self.memory.append(experience)\n","\n","    def predict(self, model, state):\n","        \"\"\" Prediction \"\"\"\n","        if model == 'online':\n","            return self.session.run(fetches=self.output, feed_dict={self.input: np.array(state)})\n","        if model == 'target':\n","            return self.session.run(fetches=self.output_target, feed_dict={self.input: np.array(state)})\n","\n","    def run(self, state):\n","        \"\"\" Perform action \"\"\"\n","        if np.random.rand() < self.eps:\n","            # Random action\n","            action = np.random.randint(low=0, high=self.actions)\n","        else:\n","            # Policy action\n","            q = self.predict('online', np.expand_dims(state, 0))\n","            action = np.argmax(q)\n","        # Decrease eps\n","        self.eps *= self.eps_decay\n","        self.eps = max(self.eps_min, self.eps)\n","        # Increment step\n","        self.step += 1\n","        return action\n","\n","    def learn(self):\n","        \"\"\" Gradient descent \"\"\"\n","        # Sync target network\n","        if self.step % self.copy == 0:\n","            self.copy_model()\n","        # Checkpoint model\n","        if self.step % self.save_each == 0:\n","            self.save_model()\n","        # Break if burn-in\n","        if self.step < self.burnin:\n","            return\n","        # Break if no training\n","        if self.learn_step < self.learn_each:\n","            self.learn_step += 1\n","            return\n","        # Sample batch\n","        batch = random.sample(self.memory, self.batch_size)\n","        state, next_state, action, reward, done = map(np.array, zip(*batch))\n","        # Get next q values from target network\n","        next_q = self.predict('target', next_state)\n","        # Calculate discounted future reward\n","        if self.double_q:\n","            q = self.predict('online', next_state)\n","            a = np.argmax(q, axis=1)\n","            target_q = reward + (1. - done) * self.gamma * next_q[np.arange(0, self.batch_size), a]\n","        else:\n","            target_q = reward + (1. - done) * self.gamma * np.amax(next_q, axis=1)\n","        # Update model\n","        summary, _ = self.session.run(fetches=[self.summaries, self.train],\n","                                      feed_dict={self.input: state,\n","                                                 self.q_true: np.array(target_q),\n","                                                 self.a_true: np.array(action),\n","                                                 self.reward: np.mean(reward)})\n","        # Reset learn step\n","        self.learn_step = 0\n","        # Write\n","        self.writer.add_summary(summary, self.step)\n","\n","    def replay(self, env, model_path, n_replay, plot):\n","        \"\"\" Model replay \"\"\"\n","        ckpt = tf.train.latest_checkpoint(model_path)\n","        saver = tf.train.import_meta_graph(ckpt + '.meta')\n","        graph = tf.get_default_graph()\n","        input = graph.get_tensor_by_name('input:0')\n","        output = graph.get_tensor_by_name('online/output/BiasAdd:0')\n","        # Replay RL agent\n","        state = env.reset()\n","        total_reward = 0\n","        with tf.Session() as sess:\n","            saver.restore(sess, ckpt)\n","            for _ in range(n_replay):\n","                step = 0\n","                while True:\n","                    time.sleep(0.05)\n","                    env.render()\n","                    # Plot\n","                    if plot:\n","                        if step % 100 == 0:\n","                            self.visualize_layer(session=sess, layer=self.conv_2, state=state, step=step)\n","                    # Action\n","                    if np.random.rand() < 0.0:\n","                        action = np.random.randint(low=0, high=self.actions, size=1)[0]\n","                    else:\n","                        q = sess.run(fetches=output, feed_dict={input: np.expand_dims(state, 0)})\n","                        action = np.argmax(q)\n","                    next_state, reward, done, info = env.step(action)\n","                    total_reward += reward\n","                    state = next_state\n","                    step += 1\n","                    if info['flag_get']:\n","                        break\n","                    if done:\n","                        break\n","        env.close()\n","\n","    def visualize_layer(self, session, layer, state, step):\n","        \"\"\" Visualization auf Conv Layers\"\"\"\n","        units = session.run(layer, feed_dict={self.input: np.expand_dims(state, 0)})\n","        filters = units.shape[3]\n","        plt.figure(1, figsize=(40, 40))\n","        n_columns = 8\n","        n_rows = np.ceil(filters / n_columns)\n","        for i in range(filters):\n","            plt.subplot(n_rows, n_columns, i+1)\n","            plt.title('Filter ' + str(i))\n","            plt.imshow(units[0, :, :, i], interpolation=\"nearest\", cmap='YlGnBu')\n","        plt.savefig(fname='./img/img-' + str(step) + '.png')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"ED-J2LE1siLd","executionInfo":{"status":"ok","timestamp":1620254472937,"user_tz":300,"elapsed":5211,"user":{"displayName":"Matthew Choi","photoUrl":"","userId":"14303575451927255395"}}},"source":["\"\"\"basic wrappers, useful for reinforcement learning on gym envs\"\"\"\n","# Mostly copy-pasted from https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n","import numpy as np\n","import os\n","os.environ.setdefault('PATH', '')\n","from collections import deque\n","import gym\n","from gym import spaces\n","import cv2\n","cv2.ocl.setUseOpenCL(False)\n","\n","\n","class NoopResetEnv(gym.Wrapper):\n","    def __init__(self, env, noop_max=30):\n","        \"\"\"Sample initial states by taking random number of no-ops on reset.\n","        No-op is assumed to be action 0.\n","        \"\"\"\n","        gym.Wrapper.__init__(self, env)\n","        self.noop_max = noop_max\n","        self.override_num_noops = None\n","        self.noop_action = 0\n","        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n","\n","    def reset(self, **kwargs):\n","        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n","        self.env.reset(**kwargs)\n","        if self.override_num_noops is not None:\n","            noops = self.override_num_noops\n","        else:\n","            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n","        assert noops > 0\n","        obs = None\n","        for _ in range(noops):\n","            obs, _, done, _ = self.env.step(self.noop_action)\n","            if done:\n","                obs = self.env.reset(**kwargs)\n","        return obs\n","\n","    def step(self, ac):\n","        return self.env.step(ac)\n","\n","\n","class FireResetEnv(gym.Wrapper):\n","    def __init__(self, env):\n","        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n","        gym.Wrapper.__init__(self, env)\n","        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n","        assert len(env.unwrapped.get_action_meanings()) >= 3\n","\n","    def reset(self, **kwargs):\n","        self.env.reset(**kwargs)\n","        obs, _, done, _ = self.env.step(1)\n","        if done:\n","            self.env.reset(**kwargs)\n","        obs, _, done, _ = self.env.step(2)\n","        if done:\n","            self.env.reset(**kwargs)\n","        return obs\n","\n","    def step(self, ac):\n","        return self.env.step(ac)\n","\n","\n","class EpisodicLifeEnv(gym.Wrapper):\n","    def __init__(self, env):\n","        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n","        Done by DeepMind for the DQN and co. since it helps value estimation.\n","        \"\"\"\n","        gym.Wrapper.__init__(self, env)\n","        self.lives = 0\n","        self.was_real_done  = True\n","\n","    def step(self, action):\n","        obs, reward, done, info = self.env.step(action)\n","        self.was_real_done = done\n","        # check current lives, make loss of life terminal,\n","        # then update lives to handle bonus lives\n","        lives = self.env.unwrapped.ale.lives()\n","        if lives < self.lives and lives > 0:\n","            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n","            # so it's important to keep lives > 0, so that we only reset once\n","            # the environment advertises done.\n","            done = True\n","        self.lives = lives\n","        return obs, reward, done, info\n","\n","    def reset(self, **kwargs):\n","        \"\"\"Reset only when lives are exhausted.\n","        This way all states are still reachable even though lives are episodic,\n","        and the learner need not know about any of this behind-the-scenes.\n","        \"\"\"\n","        if self.was_real_done:\n","            obs = self.env.reset(**kwargs)\n","        else:\n","            # no-op step to advance from terminal/lost life state\n","            obs, _, _, _ = self.env.step(0)\n","        self.lives = self.env.unwrapped.ale.lives()\n","        return obs\n","\n","\n","class MaxAndSkipEnv(gym.Wrapper):\n","    def __init__(self, env, skip=4):\n","        \"\"\"Return only every `skip`-th frame\"\"\"\n","        gym.Wrapper.__init__(self, env)\n","        # most recent raw observations (for max pooling across time steps)\n","        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n","        self._skip = skip\n","\n","    def step(self, action):\n","        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n","        total_reward = 0.0\n","        done = None\n","        for i in range(self._skip):\n","            obs, reward, done, info = self.env.step(action)\n","            if i == self._skip - 2: self._obs_buffer[0] = obs\n","            if i == self._skip - 1: self._obs_buffer[1] = obs\n","            total_reward += reward\n","            if done:\n","                break\n","        # Note that the observation on the done=True frame\n","        # doesn't matter\n","        max_frame = self._obs_buffer.max(axis=0)\n","\n","        return max_frame, total_reward, done, info\n","\n","    def reset(self, **kwargs):\n","        return self.env.reset(**kwargs)\n","\n","\n","class ClipRewardEnv(gym.RewardWrapper):\n","    def __init__(self, env):\n","        gym.RewardWrapper.__init__(self, env)\n","\n","    def reward(self, reward):\n","        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n","        return np.sign(reward)\n","\n","\n","class WarpFrame(gym.ObservationWrapper):\n","    def __init__(self, env, width=84, height=84, grayscale=True):\n","        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n","        gym.ObservationWrapper.__init__(self, env)\n","        self.width = width\n","        self.height = height\n","        self.grayscale = grayscale\n","        if self.grayscale:\n","            self.observation_space = spaces.Box(low=0, high=255,\n","                shape=(self.height, self.width, 1), dtype=np.uint8)\n","        else:\n","            self.observation_space = spaces.Box(low=0, high=255,\n","                shape=(self.height, self.width, 3), dtype=np.uint8)\n","\n","    def observation(self, frame):\n","        if self.grayscale:\n","            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n","        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n","        if self.grayscale:\n","            frame = np.expand_dims(frame, -1)\n","        return frame\n","\n","\n","class LazyFrames(object):\n","    def __init__(self, frames):\n","        \"\"\"This object ensures that common frames between the observations are only stored once.\n","        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n","        buffers.\n","        This object should only be converted to numpy array before being passed to the model.\n","        You'd not believe how complex the previous solution was.\"\"\"\n","        self._frames = frames\n","        self._out = None\n","\n","    def _force(self):\n","        if self._out is None:\n","            self._out = np.concatenate(self._frames, axis=-1)\n","            self._frames = None\n","        return self._out\n","\n","    def __array__(self, dtype=None):\n","        out = self._force()\n","        if dtype is not None:\n","            out = out.astype(dtype)\n","        return out\n","\n","    def __len__(self):\n","        return len(self._force())\n","\n","    def __getitem__(self, i):\n","        return self._force()[i]\n","\n","\n","class FrameStack(gym.Wrapper):\n","    def __init__(self, env, k):\n","        \"\"\"Stack k last frames.\n","        Returns lazy array, which is much more memory efficient.\n","        See Also\n","        --------\n","        baselines.common.atari_wrappers.LazyFrames\n","        \"\"\"\n","        gym.Wrapper.__init__(self, env)\n","        self.k = k\n","        self.frames = deque([], maxlen=k)\n","        shp = env.observation_space.shape\n","        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\n","\n","    def reset(self):\n","        ob = self.env.reset()\n","        for _ in range(self.k):\n","            self.frames.append(ob)\n","        return self._get_ob()\n","\n","    def step(self, action):\n","        ob, reward, done, info = self.env.step(action)\n","        self.frames.append(ob)\n","        return self._get_ob(), reward, done, info\n","\n","    def _get_ob(self):\n","        assert len(self.frames) == self.k\n","        return LazyFrames(list(self.frames))\n","\n","\n","def wrapper(env):\n","    \"\"\"Apply a common set of wrappers for Atari games.\"\"\"\n","    #env = EpisodicLifeEnv(env)\n","    #env = NoopResetEnv(env, noop_max=10)\n","    env = MaxAndSkipEnv(env, skip=4)\n","    if 'FIRE' in env.unwrapped.get_action_meanings():\n","       env = FireResetEnv(env)\n","    env = WarpFrame(env)\n","    env = FrameStack(env, 4)\n","    env = ClipRewardEnv(env)\n","    return env"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ny38mqE8sPJf","executionInfo":{"status":"ok","timestamp":1620254478514,"user_tz":300,"elapsed":10783,"user":{"displayName":"Matthew Choi","photoUrl":"","userId":"14303575451927255395"}},"outputId":"f0bf9140-13f5-4aa7-e998-eed48079c926"},"source":["import time\n","import numpy as np\n","!pip install nes_py\n","from nes_py.wrappers import JoypadSpace\n","!pip install gym_super_mario_bros\n","import gym_super_mario_bros as simp\n","import gym_super_mario_bros.actions as simping\n","# simping.RIGHT_ONLY, simping.COMPLEX_MOVEMENT"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nes_py in /usr/local/lib/python3.7/dist-packages (8.1.6)\n","Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes_py) (0.17.3)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes_py) (1.19.5)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes_py) (1.5.0)\n","Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes_py) (4.60.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes_py) (1.4.1)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes_py) (1.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->nes_py) (0.16.0)\n","Requirement already satisfied: gym_super_mario_bros in /usr/local/lib/python3.7/dist-packages (7.3.2)\n","Requirement already satisfied: nes-py>=8.1.2 in /usr/local/lib/python3.7/dist-packages (from gym_super_mario_bros) (8.1.6)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym_super_mario_bros) (1.19.5)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym_super_mario_bros) (1.5.0)\n","Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym_super_mario_bros) (4.60.0)\n","Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym_super_mario_bros) (0.17.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->nes-py>=8.1.2->gym_super_mario_bros) (0.16.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym_super_mario_bros) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym_super_mario_bros) (1.4.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":690},"id":"KrfFstPhs-zp","executionInfo":{"status":"error","timestamp":1620254496908,"user_tz":300,"elapsed":2004,"user":{"displayName":"Matthew Choi","photoUrl":"","userId":"14303575451927255395"}},"outputId":"b9c61cde-6f9a-4b7f-c6aa-a8dadc4f5787"},"source":["# Build env (first level, right only)\n","env = simp.make('SuperMarioBros-v0')\n","env = JoypadSpace(env, simping.RIGHT_ONLY)\n","env = wrapper(env)\n","\n","# Parameters\n","states = (84, 84, 4)\n","actions = env.action_space.n\n","\n","# Agent\n","agent = DQNAgent(states=states, actions=actions, max_memory=100000, double_q=True)\n","\n","# Episodes\n","episodes = 200\n","rewards = []\n","\n","# Timing\n","start = time.time()\n","step = 0\n","\n","# Main loop\n","for e in range(episodes):\n","\n","    # Reset env\n","    state = env.reset()\n","\n","    # Reward\n","    total_reward = 0\n","    iter = 0\n","\n","    # Play\n","    while True:\n","        \n","        # Show env\n","        env.render()\n","\n","        # Run agent\n","        action = agent.run(state=state)\n","\n","        # Perform action\n","        next_state, reward, done, info = env.step(action=action)\n","\n","        # Remember\n","        agent.add(experience=(state, next_state, action, reward, done))\n","\n","        # Replay\n","        agent.learn()\n","\n","        # Total reward\n","        total_reward += reward\n","\n","        # Update state\n","        state = next_state\n","\n","        # Increment\n","        iter += 1\n","\n","        # If done break loop\n","        if done or info['flag_get']:\n","            break\n","\n","    # Rewards\n","    rewards.append(total_reward / iter)\n","\n","    # Print\n","    if e % 100 == 0:\n","        print('Episode {e} - '\n","              'Frame {f} - '\n","              'Frames/sec {fs} - '\n","              'Epsilon {eps} - '\n","              'Mean Reward {r}'.format(e=e,\n","                                       f=agent.step,\n","                                       fs=np.round((agent.step - step) / (time.time() - start)),\n","                                       eps=np.round(agent.eps, 4),\n","                                       r=np.mean(rewards[-100:])))\n","        start = time.time()\n","        step = agent.step\n","\n","\n","\n","# Save rewards\n","# np.save('rewards.npy', rewards)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-2-2bc58f47ce82>:31: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","WARNING:tensorflow:From <ipython-input-2-2bc58f47ce82>:58: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.conv2d instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From <ipython-input-2-2bc58f47ce82>:61: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","WARNING:tensorflow:From <ipython-input-2-2bc58f47ce82>:62: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n"],"name":"stdout"},{"output_type":"error","ename":"NoSuchDisplayException","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'key'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-7bd6ae01e05f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Show env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Run agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nes_py/nes_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    374\u001b[0m                     \u001b[0mcaption\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                     \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSCREEN_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                     \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSCREEN_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                 )\n\u001b[1;32m    378\u001b[0m             \u001b[0;31m# show the screen on the image viewer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nes_py/_image_viewer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, caption, height, width, monitor_keyboard, relevant_keys)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# a mapping from pyglet key identifiers to native identifiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         self.KEY_MAP = {\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENTER\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSPACE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         }\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mimport_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pyglet.%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimport_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimport_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_module'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_doc_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""]}]},{"cell_type":"code","metadata":{"id":"k_ySIdqnfE--","executionInfo":{"status":"aborted","timestamp":1620254478772,"user_tz":300,"elapsed":11019,"user":{"displayName":"Matthew Choi","photoUrl":"","userId":"14303575451927255395"}}},"source":[""],"execution_count":null,"outputs":[]}]}